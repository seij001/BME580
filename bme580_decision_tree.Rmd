---
title: "Random Forest Model"
author: "Seijung Kim, some parts of code adapted from 'DT_RF_Demo' by Zachary Quinn"
date: "4/9/2023"
output: pdf_document
---

```{r setup, include=FALSE}
library(MASS)
library(dplyr)
library(corrplot)
library(tree)
library(randomForest)
library(stringr)
library(caret)
library(ggplot2)
library(gbm)
library(ROCR)
set.seed(2022) # used to guarantee same random values are produced when rerunning code
```

```{r}
# Load data
Data_train <- read.csv("data_train_woMiss.csv")
head(Data_train, 10)

Data_test <- read.csv("data_test_woMiss.csv")
head(Data_test, 10)
```

```{r}
# Yes = 1 & 2
# No = 0
# Unknown = Missing

# Remove missing values in the training and test sets
Data_train <- Data_train %>% filter(Stress_level != "Missing")
Data_test <- Data_test %>% filter(Stress_level != "Missing")

# Create Yes(1) and No(0) stress categories
# By replace all 2 with 1
Data_train$Stress_level <- str_replace(Data_train$Stress_level, "2", "1")
Data_test$Stress_level <- str_replace(Data_test$Stress_level, "2", "1")

Data_train # 43,716 rows
Data_test # 13,388 rows
```

```{r}
# Remove columns: X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std, Acc3_Mean, Acc3_Std, sID, date, time
Train <- subset(Data_train, select = -c(X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std,
                                Acc3_Mean, Acc3_Std, sID, date, time) )
Test <- subset(Data_test, select = -c(X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std,
                                Acc3_Mean, Acc3_Std, sID, date, time) )

# Convert data types: Stress_level to factor
Train$Stress_level <- as.factor(Train$Stress_level)
Test$Stress_level <- as.factor(Test$Stress_level)

Train
Test
```

```{r}
ncol(Train)

# We have 7 predictors (columns excluding response)
```





```{r}
# Apply 5-fold cross validation to Random Forest (to "data_train.csv")
folds <-createFolds(y=Train$Stress_level, k=5, 
                    list=TRUE, returnTrain=TRUE) # or return test
sapply(folds, length)

acc_sum = 0
auc_sum = 0

for (fold in folds){
  # Divide train and test set
  k = as.integer(sqrt(length(fold)))
  training_k = Train[fold,]
  testing_k = Train[-fold,]
  
  # Train model
  rf.pima = randomForest(Stress_level~., data = training_k, mtry = sqrt(7), ntree = 500)

  rfPreds = predict(rf.pima, testing_k)
  pred = prediction(as.numeric(rfPreds),as.numeric(testing_k$Stress_level))
  
  plot(rf.pima)  # find min error rate and return number of trees used in min error rate
  which.min(rf.pima$err.rate)
  oob.err = double(7)
  test.err = double(7)
  
  for (mtry  in 1:7){
  fit = randomForest(Stress_level~., data = training_k, mtry = mtry, ntree=500)
  oob.err[mtry] = fit$err.rate[500]
  pred = predict(fit, testing_k)
  test.err[mtry]= length(testing_k$Stress_level) - sum(ifelse(pred==testing_k$Stress_level, 1, 0))
  # changed demo code to get observations misclassified instead of correctly classified
  }

  # Iterate to find best parameters
  tuner = tuneRF(training_k[1:7], training_k$Stress_level, # specify train and test data
                stepFactor = .5, # how much you increase mtry
                plot=T,
                ntreeTry = 500,
                improve = 0.1) # oob error must decrease at least 0.1 for mtry search to continue
  plot(oob.err)
  plot(test.err)
  
  # Get AUC
  pred = prediction(as.numeric(rfPreds),as.numeric(testing_k$Stress_level))
  auc_ROCR = performance(pred, measure = "auc")
  auc_ROCR =  auc_ROCR@y.values[[1]]
  
  # Get accuracy
  tab = table(Predicted = rfPreds, Actual = testing_k$Stress_level)
  accuracy = (tab[0,0] + tab[1,1])/sum(tab)
  
  acc_sum  = acc_sum + accuracy
  auc_sum = auc_sum + auc_ROCR
}
print(acc_sum/5)
print(auc_sum/5)

```

```{r}
# Looking at the test.err graphs for all 5 runs,
# Minimum # of misclassification that mtry = 5, 6, 5, 6, 5
# Use mtry=5 first
```

```{r}
# Find training error

optRf = randomForest(Stress_level~., data = Train, mtry = 5, ntree = 500)
optPreds = predict(optRf, Train)
opt_rf_acc = sum(ifelse(optPreds==Train$Stress_level, 1, 0)) / nrow(Train)
# Make confusion matrix
table(Predicted = optPreds, Actual = Train$Stress_level)

# Accuracy and Sensitivity = 1

# Find AUC
pred = prediction(as.numeric(optPreds),as.numeric(Train$Stress_level))

auc_ROCR = performance(pred, measure = "auc")
auc_ROCR =  auc_ROCR@y.values[[1]]
  
print(auc_ROCR) # this is 1 as well

```

```{r}
# Analyze testing performance for Random Forest (if tuned well, can have much less overfitting than regular decision tree)


# Use mtry=5 for tuning the tree
optRf = randomForest(Stress_level~., data = Train, mtry = 5, ntree = 500)
optPreds = predict(optRf, Test)
opt_rf_acc = sum(ifelse(optPreds==Test$Stress_level, 1, 0)) / nrow(Test)
# Make confusion matrix
table(Predicted = optPreds, Actual = Test$Stress_level)

varImpPlot(optRf,
           sort = T,
           n.var=7,
           main = 'Top Variables')

# Determine variable importance

# For classification, use GINI (a measure of node impurity) 
importance(optRf)

hist(treesize(optRf),
     main = "No. of Nodes for the Trees",
     col = "green")

# GINI is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model. (https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1#:~:text=The%20mean%20decrease%20in%20Gini,the%20variable%20in%20the%20model.)
```

```{r}
# Calculate accuracy
# Accuracy = (true positive + true negative) / total cases = # guessed correctly/ # total data
Accuracy = (10692 + 1212) / 13847
Accuracy # 0.8534509
  
# Calculate missclassificaiton rate
# Misclassification rate = (false positive + false negative) / total cases
Miss = 1 - Accuracy
Miss # 0.1465491

# Find AUC
pred = prediction(as.numeric(optPreds),as.numeric(Test$Stress_level))

auc_ROCR = performance(pred, measure = "auc")
auc_ROCR =  auc_ROCR@y.values[[1]]
  
print(auc_ROCR)

# Sensitivity = true positive rate = TP/P_total = TP/(TP+FN)
Sensitivity = 10692/(10692+883)
Sensitivity
```


```{r}
# Calculate relative importance of sensors
results = importance(optRf)

# Combine EDA importance
EDA_sum = results['EDA_Mean',] + results['EDA_Std',] + results['EDA_Num_Peaks',]
HR_sum = results['HR_Mean',] + results['HR_Std',]
Temp_sum = results['Temp_Mean',] + results['Temp_Std',]

# Convert to relative importance
total = EDA_sum + HR_sum + Temp_sum
EDA_im = EDA_sum/total
EDA_im
HR_im = HR_sum/total
HR_im
Temp_im = Temp_sum/total
Temp_im
```



```{r}
# Analyze accuracy for Random Forest when using mtry=6

optRf = randomForest(Stress_level~., data = Train, mtry = 6, ntree = 500)
optPreds = predict(optRf, Test)
opt_rf_acc = sum(ifelse(optPreds==Test$Stress_level, 1, 0)) / nrow(Test)
# Make confusion matrix
table(Predicted = optPreds, Actual = Test$Stress_level)

varImpPlot(optRf,
           sort = T,
           n.var=7,
           main = 'Top Variables')

# Determine variable importance

# For classification, use GINI (a measure of node impurity) 
importance(optRf)

hist(treesize(optRf),
     main = "No. of Nodes for the Trees",
     col = "green")

# Conclusion: the version with mtry=6 has lower accuracy. Therefore keep using mtry=5.
```
