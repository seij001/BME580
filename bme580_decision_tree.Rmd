---
title: "Decision Tree Models"
author: "Seijung Kim, some parts of code adapted from 'DT_RF_Demo' by Zachary Quinn"
date: "4/9/2023"
output: pdf_document
---

```{r setup, include=FALSE}
library(MASS) # includes two pima datasets
library(dplyr)
library(corrplot)
library(tree)
library(randomForest)
library(stringr)
library(caret)
library(ggplot2)
library(gbm)
set.seed(2022) # used to guarantee same random values are produced when rerunning code
```

```{r}
# Load data
Data_train <- read.csv("data_train.csv")
head(Data_train, 10)

Data_test <- read.csv("data_test.csv")
head(Data_test, 10)
```

```{r}
# Yes = 1 & 2
# No = 0
# Unknown = Missing

# Before running the decision tree, need to make the stress level binary
# Make two dummy variables that represent this discrimination: 
# (a)(Yes stress + unknown) vs. no stress 
# (b) Yes stress vs. (no stress + unknown).
# Have these in separate dataframes


# Remove missing values in the training and test sets
Data_train <- Data_train %>% filter(Stress_level != "Missing")
Data_test <- Data_test %>% filter(Stress_level != "Missing")

# Create Yes(1) and No(0) stress categories
# By replace all 2 with 1
Data_train$Stress_level <- str_replace(Data_train$Stress_level, "2", "1")
Data_test$Stress_level <- str_replace(Data_test$Stress_level, "2", "1")

Data_train # 43,716 rows
Data_test # 13,388 rows
```

```{r}
# Remove columns: X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std, Acc3_Mean, Acc3_Std, sID, date, time
Train <- subset(Data_train, select = -c(X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std,
                                Acc3_Mean, Acc3_Std, sID, date, time) )
Test <- subset(Data_test, select = -c(X, Timestamp, Acc1_Mean, Acc1_Std, Acc2_Mean, Acc2_Std,
                                Acc3_Mean, Acc3_Std, sID, date, time) )

# Convert data types: Stress_level to factor
Train$Stress_level <- as.factor(Train$Stress_level)
Test$Stress_level <- as.factor(Test$Stress_level)

Train
Test

```

```{r}
ncol(Train)

# We have 7 predictors (columns excluding response)
```





```{r}
# Random Forests (version with no k-fold cross validation)

# mtry = sqrt(p) for classification
# ntree is default 500 (number of trees to grow), can increase to something like 1000
# mtry is number of variables in subset considered in each split
# got this: Warning: invalid mtry: reset to within valid range mtry = 8 (BUT WHY?)
rf.pima = randomForest(Stress_level~., data = Train, mtry = sqrt(7), ntree = 500) # 500 is default
rf.pima


rfPreds = predict(rf.pima, Test)
table(Predicted = rfPreds, Actual = Test$Stress_level)

plot(rf.pima)  # find min error rate and return number of trees used in min error rate
which.min(rf.pima$err.rate)

oob.err = double(7) # out of bag error, mtry = 8 -> OOB error = 5.53%
test.err = double(7)

# Problematic: mtree=678 is tuning model with knowledge of test data
# Need to make a second test data set
# Get all data together and run k-fold cross validation
# If 2-fold then 2 train & test sets and I can average error in between them)
# Or have train-validation-test and test set is only used for final test
for (mtry  in 1:7){
  fit = randomForest(Stress_level~., data = Train, mtry = mtry, ntree=500)
  oob.err[mtry] = fit$err.rate[500]
  pred = predict(fit, Test)
  test.err[mtry]=sum(ifelse(pred==Test$Stress_level, 1, 0))
}

# After each iter, stepFactor determines how much mtry is increased
# improve indicates the amount that oob error must decrease for the mtry search to continue
tuner = tuneRF(Train[1:7], Train$Stress_level, # specify train and test data
               stepFactor = .5, # how much you increase mtry
               plot=T,
               ntreeTry = 500,
               improve = 0.1) # oob error must decrease at least 0.1 for mtry search to continue

plot(oob.err)
plot(test.err)
```


```{r}
test.err

```

```{r}
# Analyze accuracy for Random Forest (if tuned well, can have much less overfitting than regular decision tree)

optRf = randomForest(Stress_level~., data = Train, mtry = 5, ntree = 500)
optPreds = predict(optRf, Test)
opt_rf_acc = sum(ifelse(optPreds==Test$Stress_level, 1, 0)) / nrow(Test)
table(Predicted = optPreds, Actual = Test$Stress_level)

varImpPlot(optRf,
           sort = T,
           n.var=7,
           main = 'Top Variables')

# Determine variable importance

# For classification, use GINI (a measure of node impurity) 
importance(optRf)

hist(treesize(optRf),
     main = "No. of Nodes for the Trees",
     col = "green")

# GINI is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model. (https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1#:~:text=The%20mean%20decrease%20in%20Gini,the%20variable%20in%20the%20model.)
```

```{r}
# Calculate accuracy
# Accuracy = (true positive + true negative) / total cases = # guessed correctly/ # total data
Accuracy = (10335 + 1091) / 13388
Accuracy # 0.8534509
  
# Calculate missclassificaiton rate
# Misclassification rate = (false positive + false negative) / total cases
Miss = 1 - Accuracy
Miss # 0.1465491
```


```{r}
# Apply K-fold cross validation to Random Forest (to "data_train.csv")
# k = 5
# Use "data_test.csv" for the final testing and report accuracy for this dataset

# https://stackoverflow.com/questions/66711347/random-forest-oob-for-k-fold-cross-validation
# Define cross validation with 5 folds (https://www.statology.org/k-fold-cross-validation-in-r/)
ctrl <- trainControl(method= 'cv', number=5)

# Random Forest Model
rf.pima = randomForest(Stress_level~., data = Train, mtry = sqrt(7), ntree = 500) # 500 is default
rf.pima


## Train a random forest model
forest <- train(
        Stress_level~., 
        data=Train, 
        
        # `rf` method for random forest
        method='rf', 
        
        # Add repeated cross validation as trControl
        trControl=repeat_cv,
        
        # Accuracy to measure the performance of the model
        metric='Accuracy')

## Print out the details about the model
forest$finalModel
```
