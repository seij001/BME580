---
title: "SVM Demo"
author: "Achudh Balaraman"
date: "3/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
# Hi everyone, today we're going to talk a little bit about how we can create support vector machines, or SVMs, in R
library(e1071) # Includes SVM function
library(caret) # Includes functions we'll use for hyperparameter tuning
library(tidyverse)
library(gmodels) # CrossTable()

```

```{r}
set.seed(2022)
train = read.csv("data_train.csv")
test = read.csv("data_test.csv")
```

```{r}
train <- train %>% filter(Stress_level != "Missing")
train <- subset(train, select = c(EDA_Mean, EDA_Std, EDA_Num_Peaks, HR_Mean, HR_Std, Temp_Mean, Temp_Std, Stress_level))
test <- test %>% filter(Stress_level != "Missing")
test <- subset(test, select = c(EDA_Mean, EDA_Std, EDA_Num_Peaks, HR_Mean, HR_Std, Temp_Mean, Temp_Std, Stress_level))
train$Stress_level <- replace(train$Stress_level,train$Stress_level == 2,1)
test$Stress_level <- replace(test$Stress_level,test$Stress_level == 2,1)
train = train %>% mutate(across(-Stress_level, scale)) %>% mutate_at("Stress_level", as.factor)
test = test %>% mutate(across(-Stress_level, scale)) %>% mutate_at("Stress_level", as.factor)

#trainst = train %>% dplyr::select(-c('Stress_level'))
# trainMean = apply(train%>% dplyr::select(-c('Stress_level')) ,2,mean)
# trainSd = apply(train%>% dplyr::select(-c('Stress_level')) ,2,sd)

#trainst = sweep(sweep(trainst, 2L, trainMean), 2, trainSd, "/")
#summary(trainst)
#testst = sweep(sweep(test %>% dplyr::select(-c('Stress_level')), 2L, trainMean), 2, trainSd, "/")

#trainst$Stress_level = train$Stress_level
#testst$Stress_level = test$Stress_level
head(train)
head(test)
```

```{r}


# SVMs work by projecting each observation into a higher dimensional space, then
# identifying a hyperplane capable of separating the classes. As a result, the
# model can be negatively impacted by differing measurement scales between
# features. We can solve this by scaling our data

#split_ind = createDataPartition(data$Class, p=0.8, list = F)
#train = scaleDf[split_ind,]
#test = scaleDf[-split_ind,]


svmMod = svm(Stress_level~., data = train, type = 'C', cost = 1,
             kernel =  'linear', cross = 5)

# We can plot a 2D representation of the results using the plot function
```
```{r}
plot(x=svmMod, data = train, formula = EDA_Mean~Temp_Mean)
```
```{r}
# We have a hyperparameter here called cost, which we likely want to tune in
# order to find the optimal value. Let's do that with 10 fold cross validation.
# The caret library has a handy function for a lot of hyperparameter tuning
# called trainControl repeats is an argument specific to repeated cv which
# determines the number of complete sets of folds to compute, while number is
# the number of folds to cross-validate with. So we cross-validate with 10 folds,
# 3 times
stressVal = trainControl(method = 'repeatedcv',
                       repeats = 3,
                       number = 5)


svmCv = train(Stress_level~., 
              data = train,
              method = 'svmLinear',
              trControl = stressVal,
              tuneGrid = expand.grid(C=c(10^seq(from = -3, to = 2, by = .2))),
              metric = 'Kappa')

# Here we see a couple of values, the cost, the accuracy associated with that
# cost, and kappa which is Cohen's kappa. Briefly, Cohen's kappa is a metric
# that compares observed accuracy with expected accuracy. In the case of this
# dataset, we have a 33% chance of guessing the species if we randomly guessed
# (expected accuracy), cohens kappa gives us a measure of how much better our
# observed accuracy is than that. It is always between 0 and 1, and the closer
# to 1 the better your model is than random chance.

# To obtain the cost value associated with the best model, we can index into the
# bestTune value
optimal_stress = svmCv$bestTune
# We see that c = ~1.6 gives us the optimal model

# Using the predict function on a cross-validated caret model will by default use
# the parameters that created the best model in the training data
originalSVMPred = predict(svmMod, test, type = 'class')
optSVMPred = predict(svmCv, test, type = 'raw')

CrossTable(originalSVMPred, test$Stress_level)
CrossTable(optSVMPred, test$Stress_level)

# SVMs have a lot of benefits, one of which is that they can use what's called a
# kernel trick to create a non-linear decision boundary. Commonly, this takes
# the form of a radial basis kernel. This is especially helpful if you linear
# SVM doesn't perform well and you believe the data may not be linearly
# separable.

# When training an SVM with a radial kernel, we have an additional
# hyperparameter to tune: gamma (also called sigma) which is the free parameter
# in the radial basis kernel - essentially it states how flexible the decision
# boundary can be

# To identify the optimal hyperparameters, we first set up a grid of potential
# values
optParamGrid = expand.grid(C=c(.001, .01, 1, 5, 10, 20, 50, 100),
                                            sigma = c(0.5, 1, 2, 3, 4))

optRadSVM = train(Stress_level~.,
                  train,
                  method = 'svmRadial',
                  tuneGrid = optParamGrid,
                  trControl = stressVal,
                  metric = 'Accuracy') 
print(optRadSVM)
optRadSVM$bestTune

optRadPred = predict(optRadSVM, test, type = 'raw')
CrossTable(optRadPred, test$Stress_level)
```


